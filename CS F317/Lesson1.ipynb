{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfu9uwKorWp6"
   },
   "source": [
    "## **Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDkL7M6krc68"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1qc-8-xrf5I"
   },
   "source": [
    "## **Creating the plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvSAn9eKrix1"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  rewards = history[\"rewards\"]\n",
    "  cum_rewards = history[\"cum_rewards\"]\n",
    "  chosen_arms = history[\"arms\"]\n",
    "\n",
    "  fig = plt.figure(figsize=[30,8])\n",
    "\n",
    "  ax2 = fig.add_subplot(121)\n",
    "  ax2.plot(cum_rewards, label=\"avg rewards\")\n",
    "  ax2.set_title(\"Cummulative Rewards\")\n",
    "\n",
    "  ax3 = fig.add_subplot(122)\n",
    "  ax3.bar([i for i in range(len(chosen_arms))], chosen_arms, label=\"chosen arms\")\n",
    "  ax3.set_title(\"Chosen Actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiW4JZ5yrmKd"
   },
   "source": [
    "## **Creating Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPnkfiuurpK_"
   },
   "outputs": [],
   "source": [
    "class Env(object):\n",
    "    \n",
    "    # setting all the parameters as member variables of class\n",
    "    def __init__(self, reward_probabilities, rewards):\n",
    "        self.reward_probabilities = reward_probabilities    # probability of receiving reward from that arm\n",
    "        self.rewards = rewards                              # acquired reward from that arm\n",
    "        self.k_arms = len(rewards)                          # number of arms\n",
    "        \n",
    "        \n",
    "    def choose_arm(self, arm):\n",
    "        if np.random.random() < self.reward_probabilities[arm]:\n",
    "            return self.rewards[arm]\n",
    "        else:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQx6_tCNvBBn"
   },
   "source": [
    "## **Instantiating the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvF_M4eLvFTD"
   },
   "outputs": [],
   "source": [
    "\n",
    "reward_probabilities = [0.01, 1.0, 0.75, 0.99, 0.65, 1.0];\n",
    "rewards = [95.0, 0.0, 25.5, 10.05, 5.45, 2.50];\n",
    "environment = Env(reward_probabilities, rewards)\n",
    "\n",
    "print(f\"K_arms \\t\\t\\t: {environment.k_arms}\")\n",
    "print(f\"Reward probabilities \\t: {environment.reward_probabilities}\")\n",
    "print(f\"Rewards \\t\\t: {environment.rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWyzYRg9wQS4"
   },
   "source": [
    "## **Performing a selective action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpFDJuYEwXGe"
   },
   "outputs": [],
   "source": [
    "[environment.choose_arm(2) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LcqmTA7xBAb"
   },
   "source": [
    "## **Balancing Exploration and Exploitation with epsilon greedy algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff8x5uwHxSLF"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(object):\n",
    "    \n",
    "    # setting all the parameters as member variables of class\n",
    "    def __init__(self, env, max_iterations=200, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.iterations = max_iterations\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    # method that let the agent act within the environment\n",
    "    def act(self):\n",
    "        q_values = np.zeros(self.env.k_arms)       # Payout of each arm is set to zero\n",
    "        arm_rewards = np.zeros(self.env.k_arms)    # Total rewards of each arm is set to zero\n",
    "        arm_counts = np.zeros(self.env.k_arms)     # Number of times each arm is pulled\n",
    "        \n",
    "        rewards = []        # list to store the actual rewards that agent makes\n",
    "        cum_rewards = []    # Average of all the rewards\n",
    "        \n",
    "        \n",
    "        for i in range(1, self.iterations+1):    # choose action using epsilon greedy algorithm\n",
    "            if np.random.random() < self.epsilon:    # random action/exploration\n",
    "                arm = np.random.choice(self.env.k_arms)\n",
    "                \n",
    "            else:                                    # greedy action/exploitation\n",
    "                arm = np.argmax(q_values)            # argmax has a property that if 2 index has same Qvalue then it chooses the lower index always\n",
    "                \n",
    "            reward = self.env.choose_arm(arm)\n",
    "\n",
    "            arm_rewards[arm] += reward                       # update the values\n",
    "            arm_counts[arm] += 1\n",
    "            q_values[arm] = arm_rewards[arm]/arm_counts[arm]\n",
    "\n",
    "            rewards.append(reward)\n",
    "            cum_rewards.append(sum(rewards)/len(rewards))    # append the values in list\n",
    "    \n",
    "        return {\"arms\": arm_counts, \"rewards\": rewards, \"cum_rewards\": cum_rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlZwXleMzzNK"
   },
   "source": [
    "## **Implementing Epsilon greedy algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2PJ1SAdz3vx"
   },
   "outputs": [],
   "source": [
    "egreedy_agent = EpsilonGreedyAgent(environment, max_iterations=2000, epsilon=0.1)    # instantiate the class\n",
    "eg_history = egreedy_agent.act()                                                     # make the agent to act\n",
    "print(f\"TOTAL REWARD : {sum(eg_history['rewards'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlrC12M_2yVs"
   },
   "source": [
    "## **Mapping plot for Epsilon greedy algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkW3zlAG23MW"
   },
   "outputs": [],
   "source": [
    "plot_history(eg_history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
